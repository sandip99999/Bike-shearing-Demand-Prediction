{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "HhfV-JJviCcP",
        "nA9Y7ga8ng1Z",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "GF8Ens_Soomf",
        "HmyMsH5Ypllj",
        "xiyOF9F70UgQ",
        "QIKM3mYmUy7y",
        "id1riN9m0vUs",
        "TP7f1HdGyvdK",
        "89xtkJwZ18nB",
        "BhH2vgX9EjGr",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "xswJzNwGvtMG",
        "oeDJ5wgnJ5iU",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandip99999/Bike-shearing-Demand-Prediction/blob/main/Copy_of_Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Seoul Bike Sharing Demand Prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - **Regression**\n",
        "\n",
        "##### **Contribution**    - **Individual**\n",
        "\n",
        "##### **Team Member**- **Sandip Dey**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The contents of the data came from a city called Seoul. A bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system. The data had variables such as date, hour, temperature, humidity, wind-speed, visibility, dew point temperature, solar radiation, rainfall, snowfall, seasons, holiday, functioning day and rented bike count.\n",
        "\n",
        "\n",
        "The problem statement was to build a machine learning model that could predict the rented bikes count required for an hour, given other variables. The first step in the exercise involved exploratory data analysis where we tried to dig insights from the data in hand. It included univariate and multivariate analysis in which we identified certain trends, relationships, correlation and found out the features that had some impact on our dependent variable. The second step was to clean the data and perform modifications. We checked for missing values and outliers and removed irrelevant features. We also encoded the categorical variables. The third step was to try various machine learning algorithms on our split and standardized data. We tried different algorithms namely; Linear regression, Randomforest and XGBoost. We did hyperparameter tuning and evaluated the performance of each model using various metrics. The best performance was given by the Gradient boosting and Random forest model where the R2_score for training and test set was 0.95 and 0.92 respectively.\n",
        "\n",
        "\n",
        "The most important features who had a major impact on the model predictions were; hour, temperature, wind-speed, solar-radiation, month and seasons. Demand for bikes got higher when the temperature and hour values were more. Demand was high for low values of wind-speed and solar radiation. Demand was high during springs and summer and very low during winters.\n",
        "\n",
        "\n",
        "The model performed well in this case but as the data is time dependent, values of temperature, wind-speed, solar radiation etc. will not always be consistent. Therefore, there will be scenarios where the model might not perform well. As Machine learning is an exponentially evolving field, we will have to be prepared for all contingencies and also keep checking our model from time to time\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/sandip99999/Bike-shearing-Demand-Prediction"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd     #(provides wide variety tools for data analysis,many inbuilt methods for grouping,)\n",
        "                         #(combining and filtering data.)\n",
        "    \n",
        "import numpy as np      #for some basic mathematical operations\n",
        "\n",
        "from matplotlib import pyplot as plt #comprehensive library for creating static, animated, and interactive visualizations\n",
        "\n",
        "import seaborn as sns                #  high-level interface for drawing attractive and informative statistical graphics\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV,  cross_val_score\n",
        "from sklearn import preprocessing, linear_model\n",
        "from sklearn.preprocessing import  LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler \n",
        "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score\n",
        "from sklearn.linear_model import Ridge, Lasso, LinearRegression, LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
        "from sklearn import neighbors\n",
        "from sklearn.svm import SVR\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "pd.pandas.set_option('display.max_columns',None)\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"/content/drive/MyDrive/Colab Notebooks/Bike shearing demand prediction/SeoulBikeData.csv\"\n",
        "bike_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Bike shearing demand prediction/SeoulBikeData.csv\",encoding= 'unicode_escape')"
      ],
      "metadata": {
        "id": "SlMPUkVdZegl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "bike_df.head(5)\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.tail(5)"
      ],
      "metadata": {
        "id": "KOih3PcumnhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting all the columns\n",
        "print(\"Features of the dataset:\")\n",
        "bike_df.columns"
      ],
      "metadata": {
        "id": "wLlDm8iFm0_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "bike_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'There are {bike_df.shape[0]} Rows and {bike_df.shape[1]} Columns in the dataset')"
      ],
      "metadata": {
        "id": "4gp_Fs8ZhHYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "bike_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(\"Duplicate entry in dataset is :\",bike_df.duplicated().sum())"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Not found any duplicated entries inside the dataset**"
      ],
      "metadata": {
        "id": "fbXF3K-RiYbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "bike_df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "bike_df.isna().sum()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame((bike_df.isnull().sum())*100/bike_df.shape[0]).reset_index()"
      ],
      "metadata": {
        "id": "MwOvN7wGrWku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing = pd.DataFrame((bike_df.isnull().mean())*100).reset_index()\n",
        "plt.figure(figsize=(8,3))\n",
        "ax = sns.pointplot(data=missing,x=\"index\",y=0)\n",
        "plt.xticks(rotation =90,fontsize =7)\n",
        "plt.title(\"Percentage of Missing values\")\n",
        "plt.ylabel(\"PERCENTAGE\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4Qe-fwFRpYo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Not found any missing values inside the dataset**"
      ],
      "metadata": {
        "id": "_2TaaZ7elKiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Function for Dtype,Unique values and Null values\n",
        "def bike_df_info():\n",
        "    bike = pd.DataFrame(index=bike_df.columns)\n",
        "    bike['DataType'] = bike_df.dtypes\n",
        "    bike[\"Non-null_Values\"] = bike_df.count()\n",
        "    bike['Unique_Values'] = bike_df.nunique().sort_values(ascending=True)\n",
        "    bike['NaN_Values'] = bike_df.isnull().sum()\n",
        "    bike['NaN_Values_Percentage'] = (bike['NaN_Values']/len(bike_df))*100 \n",
        "    return bike"
      ],
      "metadata": {
        "id": "Y-kQwcJTlqVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Function\n",
        "bike_df_info()"
      ],
      "metadata": {
        "id": "dg3ZFTeKmYRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding details from data:**\n",
        "\n",
        "1. There are 14 features with 8760 rows of data.\n",
        "2. There are 4 categorical columns and 10 numerical columns. Columns ‚ÄòDate‚Äô, ‚ÄòSeasons‚Äô and ‚ÄòFunctioning Day‚Äô are of ùëúùëèùëóùëíùëêùë° data type\n",
        "3. Columns ‚ÄòRented Bike Count‚Äô, ‚ÄòHour‚Äô, ‚ÄòHumidity (%)' and ‚ÄòVisibility (10ùëö)' are of ùëñùëõùë°64 numarical data type\n",
        "4. Columns ‚ÄòTemperature Temperature (‚ÑÉ)‚Äô, ‚ÄòWind Speed (ùëö/ùë†)‚Äô, ‚ÄòDew Point Temperature (‚ÑÉ)‚Äô,‚ÄòSolar Radiation (ùëÄùêΩ/ùëö2)‚Äô,‚ÄòRainfall (ùëöùëö)' and ‚ÄòSnowfall(ùëêùëö) are of ùëìùëôùëúùëéùë°64 numarical data type\n",
        "5. Not any null value present in any column\n",
        "6. Unique count : Seasons- 4 , Holiday- 2 , Functioning Day- 2, Date-365, Rented Bike Count-2166, Hour-24, Temperature(¬∞C)-546 etc."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(\"Features of the dataset:\")\n",
        "bike_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "bike_df.describe().T               #.T use for transpose the describe table"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Breakdown of Our Features:**\n",
        "\n",
        "**Date** : *The date of the day, during 365 days from 01/12/2017 to 30/11/2018, formating in DD/MM/YYYY, type : str*, we need to convert into datetime format.\n",
        "\n",
        "**Rented Bike Count** : *Number of rented bikes per hour which our dependent variable and we need to predict that, type : int*\n",
        "\n",
        "**Hour**: *The hour of the day, starting from 0-23 it's in a digital time format, type : int, we need to convert it into category data type.*\n",
        "\n",
        "**Temperature(¬∞C)**: *Temperature in Celsius, type : Float*\n",
        "\n",
        "**Humidity(%)**: *Humidity in the air in %, type : int*\n",
        "\n",
        "**Wind speed (m/s)** : *Speed of the wind in m/s, type : Float*\n",
        "\n",
        "**Visibility (10m)**: *Visibility in m, type : int*\n",
        "\n",
        "**Dew point temperature(¬∞C)**: *Temperature at the beggining of the day, type : Float*\n",
        "\n",
        "**Solar Radiation (MJ/m2)**: *Sun contribution, type : Float*\n",
        "\n",
        "**Rainfall(mm)**: *Amount of raining in mm, type : Float*\n",
        "\n",
        "**Snowfall (cm)**: *Amount of snowing in cm, type : Float*\n",
        "\n",
        "**Seasons**: *Season of the year, type : str, there are only 4 season's in data *. \n",
        "\n",
        "**Holiday**: *If the day  is holiday period or not, type: str*\n",
        "\n",
        "**Functioning Day**: *If the day is a Functioning Day or not, type : str*\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y1dvI5gGopUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking the possible values important and meaningful categorical columns can have**"
      ],
      "metadata": {
        "id": "byTyHtpU0ld3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(bike_df[\"Seasons\"].unique())"
      ],
      "metadata": {
        "id": "BnlRZrccx92b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bike_df[\"Holiday\"].unique())"
      ],
      "metadata": {
        "id": "PAW0PbbOx9_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bike_df[\"Functioning Day\"].unique())"
      ],
      "metadata": {
        "id": "FJi_rosEx-I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "#Rename the complex columns name\n",
        "bike_df=bike_df.rename(columns={'Rented Bike Count':'Rented_Bike_Count',\n",
        "                                'Temperature(¬∞C)':'Temperature',\n",
        "                                'Humidity(%)':'Humidity',\n",
        "                                'Wind speed (m/s)':'Wind_speed',\n",
        "                                'Visibility (10m)':'Visibility',\n",
        "                                'Dew point temperature(¬∞C)':'Dew_point_temperature',\n",
        "                                'Solar Radiation (MJ/m2)':'Solar_Radiation',\n",
        "                                'Rainfall(mm)':'Rainfall',\n",
        "                                'Snowfall (cm)':'Snowfall',\n",
        "                                'Functioning Day':'Functioning_Day'})"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.head(2)"
      ],
      "metadata": {
        "id": "5FzueG_ZRya2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.groupby(\"Functioning_Day\")[\"Rented_Bike_Count\"].sum().sort_values(ascending=False).reset_index()"
      ],
      "metadata": {
        "id": "9yIAGgvJi93E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As per diagnosis data found that rental bike only given on Functioning Day,So remove Functioning Day Column."
      ],
      "metadata": {
        "id": "3efEfDylkgPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Due to not unsefull in Functioning Day Column ,remove Functioning Day Column\n",
        "bike_df1=bike_df.drop([\"Functioning_Day\"],axis=1)"
      ],
      "metadata": {
        "id": "qsNfSaR1kyPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the new shape of the dataset\n",
        "print(f\"New shape of the dataset is {bike_df1.shape}\")"
      ],
      "metadata": {
        "id": "1oMcRHx-lGIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert datetime to datatype\n",
        "import datetime as dt\n",
        "bike_df1['Date'] = bike_df1['Date'].apply(lambda x: dt.datetime.strptime(x,\"%d/%m/%Y\"))"
      ],
      "metadata": {
        "id": "sg-a5yfXRymL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Seperate Day, Month, Year from DataFrame Column\n",
        "bike_df1['Day']=bike_df1['Date'].dt.day_name()\n",
        "bike_df1['Month']=bike_df1['Date'].dt.month\n",
        "bike_df1['Year']=bike_df1['Date'].dt.year"
      ],
      "metadata": {
        "id": "pwlaqhn5o88n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1.head(10)"
      ],
      "metadata": {
        "id": "Wpm34T72o8_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1[\"Year\"].unique()"
      ],
      "metadata": {
        "id": "YLexfFMNCM9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a new column of \"weekdays_weekend\" and drop the column \"Date\",\"day\",\"year\"\n",
        "bike_df1['weekdays_weekend']=bike_df1['Day'].apply(lambda x : 1 if x=='Saturday' or x=='Sunday' else 0 )\n",
        "bike_df1=bike_df1.drop(columns=['Date','Day','Year'],axis=1)"
      ],
      "metadata": {
        "id": "w-gF9Euqo9CA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1.head(2)"
      ],
      "metadata": {
        "id": "L-0YFPqJo9Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1.info()"
      ],
      "metadata": {
        "id": "5pwToWw6o9I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1['weekdays_weekend'].value_counts()"
      ],
      "metadata": {
        "id": "cqUaH5ngs0aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Change the int64 column into catagory column\n",
        "columns1=['Hour','Seasons','Holiday','Month','weekdays_weekend']\n",
        "for col in columns1:\n",
        "  bike_df1[col]=bike_df1[col].astype('category')"
      ],
      "metadata": {
        "id": "PslZr3ges0hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Change the int64 column into float column\n",
        "columns2=[\"Rented_Bike_Count\",\"Humidity\",\"Visibility\"]\n",
        "for col in columns2:\n",
        "  bike_df1[col]=bike_df1[col].astype('float')"
      ],
      "metadata": {
        "id": "7Hu0lVWCWfKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's check the result of data type\n",
        "bike_df1.info()"
      ],
      "metadata": {
        "id": "iQjJ8D3us0rC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1.columns"
      ],
      "metadata": {
        "id": "hqO3QCwmt6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. As per diagnosis data found that rental bike only given on Functioning Day,  remove Functioning Day Column.\n",
        "2. So we convert the \"date\" column into 3 different column i.e \"year\",\"month\",day\".\n",
        "3. The \"year\" column in our data set is basically contain the 2 unique number contains the details of from 2017 december to 2018 november so if i consider this is a one year then we don't need the \"year\" column so we drop it.\n",
        "4. The other column \"day\", it contains the details about the each day of the month, for our relevence we don't need each day of each month data but we need the data about, if a day is a weekday or a weekend so we convert it into this format and drop the \"day\" column.\n",
        "5. As \"Hour\",\"month\",\"weekdays_weekend\" column are show as a integer data type but actually it is a category data tyepe. so we need to change this data tyepe if we not then, while doing the further anlysis and correleted with this then the values are not actually true so we can mislead by this."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's explore the data by visualizing the distribution of values in some columns of the dataset, and the relationships between \"Rented Bike count\" and other columns.**\n",
        "\n",
        "**We'll use libraries Matplotlib, Seaborn for visualization.**"
      ],
      "metadata": {
        "id": "QxeW4N-yzTWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 Relation between 'Month' and 'Rented_bike_count'"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1.head(2)"
      ],
      "metadata": {
        "id": "ndiXBkOO8zZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "fig,ax=plt.subplots(figsize=(14,7))\n",
        "sns.barplot(data=bike_df1,x='Month',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Month ')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the distribution of the Rented_bike_count in each month  "
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above bar plot we can clearly say that from  the month May to October the demand of the rented bike is high as compare to other months. \n",
        "\n",
        "These months are comes inside the summer season."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the summer season the rented bike business will go high as compare to winter season."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 Count of Rented bikes according to Weekdays_Weekend and Time"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1.head(2)"
      ],
      "metadata": {
        "id": "BRGuL5n28xNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean distribution of Rented bike betwwn weekdays and weeend\n",
        "bike_df1.groupby(\"weekdays_weekend\")[\"Rented_Bike_Count\"].mean().reset_index()"
      ],
      "metadata": {
        "id": "lxFLqCmV4YDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "fig,ax=plt.subplots(figsize=(10,8))\n",
        "sns.barplot(data=bike_df1,x='weekdays_weekend',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes according to weekdays and weekend ')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "fig,ax=plt.subplots(figsize=(14,7))\n",
        "sns.pointplot(data=bike_df1,x='Hour',y='Rented_Bike_Count',hue='weekdays_weekend',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to weekdays_weekend ')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the distribution of use of Rented bikes acording to weekdays_weekend and time ."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. From the above bar plot we can clearly say that the mean distribution of rented bike between Weekdays and weekend is almost same.\n",
        "  But in weekdays its slightly higher due to office and weekend its slightly lower.\n",
        "\n",
        "2. From the above point plot and bar plot we can say that in the week days which represent in blue colur show that the demand of the bike higher because of the office.\n",
        "\n",
        "  Peak Time are 7 am to 9 am and 5 pm to 7 pm\n",
        "\n",
        "\n",
        "3. The orange colur represent the weekend days, and it show that the demand of rented bikes are very low specially in the morning hour but when the evening start from 4 pm to 8 pm the demand slightly increases.  "
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above graph we can say that we can run the business in entire week. and It will not affect our profit margins.\n",
        "\n",
        "Weekdays in office time between 7 am to 9 am and 5pm to 7pm the demand of rented bike increases \n",
        "\n",
        "Similarly in weekend from 4pm to 8pm the demand slightly increases compare to others time."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 Relation between Rented Bikes count and Hours"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1.head(2)"
      ],
      "metadata": {
        "id": "OPM8G0Ku8raO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(14,7))\n",
        "sns.barplot(data=bike_df,x='Hour',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Hour ')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9LFodBnQAoMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the use of rented bike according the hours."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above plot which shows the use of rented bike according the hours and the data are from all over the year.generally people use rented bikes during their working hour from 7am to 9am and 5pm to 7pm.  "
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above graph which show that maximum demend of rented bikes comes at the time working hour from 7am to 9am and 5pm to 7pm and \n",
        "\n",
        "minimum demend of rented bikes comes in the morning ."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 Count of Rented bikes according to Seasons"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1.head(2)"
      ],
      "metadata": {
        "id": "Sn6nMdYkDiJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "fig,ax=plt.subplots(figsize=(14,7))\n",
        "sns.barplot(data=bike_df1,x=\"Seasons\",y=\"Rented_Bike_Count\",ax=ax,capsize=.2)\n",
        "ax.set(title=\"Count of Rented bikes according to Seasons\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "fig,ax=plt.subplots(figsize=(14,7))\n",
        "sns.pointplot(data=bike_df1,x='Hour',y='Rented_Bike_Count',hue='Seasons',ax=ax)\n",
        "ax.set(title='Count of Rented bikes according to Seasons')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xgMnM6MlEvrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the distribution of use of Rented bikes acording to Seasons ."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above bar plot and point plot which shows the use of rented bike in in four different seasons, and it clearly shows that,\n",
        "\n",
        "1. In summer season the use of rented bike is high and peak time is 7am-9am and 7pm-5pm.\n",
        "\n",
        "2. In winter season the use of rented bike is very low because of snowfall."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summer, Spring and Autumn seasons the use of rented bike are high so it is profitable to do business in that time.\n",
        "And at the time of summer we will get maxium profit.\n",
        "\n",
        "In winter season the use of rented bike is very low because of snowfall. Due to that we will get minimum profit mergin."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 Analyze of Numerical variables"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "bike_df1.head(2)"
      ],
      "metadata": {
        "id": "BK_MR_A7Ifso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1.info()"
      ],
      "metadata": {
        "id": "f0NlskJLKhEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if dtype is not Equal to object type then its a num data\n",
        "numerical_features=[col for col in bike_df1.columns if bike_df1[col].dtype==\"float\"]\n",
        "numerical_features"
      ],
      "metadata": {
        "id": "B0gcUsH5KNlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seprate dataframe for Numerical feature\n",
        "num_data=bike_df1[numerical_features]\n",
        "num_data.head(5)"
      ],
      "metadata": {
        "id": "tAQj1PdSXb7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in numerical_features:\n",
        "    fig = plt.figure(figsize=(6, 3))\n",
        "    ax = fig.gca()\n",
        "    feature = bike_df1[col]\n",
        "    sns.distplot(feature)\n",
        "    ax.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "    ax.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)    \n",
        "    ax.set_title(col)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2-L0PTV1sPkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For and check distribution of numerical and analyse it in the dataset. "
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.From the above plot which shows that mean ranted bike count was 650 and ranted bike count maximum goes to above 3000 in a day.and this is right skew distribution.\n",
        "\n",
        "2.From the above plot which shows that mean temperatur was 14 degree and the maximum distribution lies between 0 to 30 degree celsius.\n",
        "\n",
        "3.From the above plot which shows that mean Humidity was 58.\n",
        "\n",
        "4.From the above plot which shows that mean wind speed in the year was 1.7 m/s and its normal.\n",
        "\n",
        "5.From the above plot which shows that the maximum days visibility was good and the mean visibility in the year was 1700. \n",
        "\n",
        "6.From the above plot which shows that the mean Dew poin temperation was 5 degC.and the maximum distribution lies between -5 to +25 deg \n",
        "\n",
        "7.From the above plot which shows that mean solar radiation lies about 0.6 and maimum days solar radiation lies close to zero.\n",
        "\n",
        "8.From the above plot which shows that in a year maximum days were dry.\n",
        "\n",
        "9.From the above plot which shows that in a year maximum days sky was clear and did not have snowfall."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1.agg(['skew']).T"
      ],
      "metadata": {
        "id": "qQOxNAXizSM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Right/Positive Skewed Distribution: Mode < Median < Mean: Rented Bike Count, Wind Speed, Solar Radiation\n",
        "\n",
        "2. No Skew: Mean = Median = Mode : Hour, Temperature, Humidity(%),Rainfall(mm),Snowfall(cm)\n",
        "\n",
        "3. Left/Negative Skewed Distribution: Mean < Median < Mode: visibility(10m),Teperature"
      ],
      "metadata": {
        "id": "wIrfMoJkzdbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6  Numerical vs.Rented_Bike_Count"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "bike_df1.head(2)"
      ],
      "metadata": {
        "id": "6h63ZxNEks83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Temperature\"  \n",
        "bike_df1.groupby(\"Temperature\")[\"Rented_Bike_Count\"].mean().plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between Humidity and Rented_Bike_Count\n",
        "bike_df1.groupby(\"Humidity\")[\"Rented_Bike_Count\"].mean().plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D2BVCeMr9M4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between Wind_speed and Rented_Bike_Count\n",
        "bike_df1.groupby(\"Wind_speed\")[\"Rented_Bike_Count\"].mean().plot()"
      ],
      "metadata": {
        "id": "QHWLkS0R9M7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between Visibility and Rented_Bike_Count\n",
        "bike_df1.groupby(\"Visibility\")[\"Rented_Bike_Count\"].mean().plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fnyctJ6U9M-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between Dew_Point_Teperatue and Rented_Bike_Count\n",
        "bike_df1.groupby(\"Dew_point_temperature\")[\"Rented_Bike_Count\"].mean().plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wxarAJFU9NAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between Solar_Radiation and Rented_Bike_Count\n",
        "bike_df1.groupby(\"Solar_Radiation\")[\"Rented_Bike_Count\"].mean().plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e45TFJ2o9ND5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between Rainfall and Rented_Bike_Count\n",
        "bike_df1.groupby(\"Rainfall\")[\"Rented_Bike_Count\"].mean().plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yq9qFdjn9NGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between Snowfall and Rented_Bike_Count\n",
        "bike_df1.groupby(\"Snowfall\")[\"Rented_Bike_Count\"].mean().plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6-CdDU5n9NJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For established relation between Numerical data and Rented_Bike_Count"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. From the above plot we see that people like to ride bikes when it is pretty hot around 25¬∞C in average\n",
        "\n",
        "2. We can see from the above plot that the demand of rented bike is uniformly distribute from 20% to 80% Humidity but when the Humidity was above 80% then the demand of rented bike decrease and below 20% Humidity the demand of rented bike was increased.\n",
        "\n",
        "\n",
        "3. We can see from the above plot that the demand of rented bike is uniformly distribute despite of wind speed but when the speed of wind was 7 m/s then the demand of rented bike also increase that clearly means peoples love to ride bikes when its little windy.\n",
        "\n",
        "4. We can see from the above plot that the demand of rented bike is uniformly distribute above 500 visibility but below 500 visibility the demand of rented bike slightly less. \n",
        "\n",
        "5. From the above plot of \"Dew_point_temperature' is almost same as the 'temperature' there is some similarity present we can check it in our next step.\n",
        "\n",
        "\n",
        "6. from the above plot we see that, the amount of rented bikes is huge, when there is solar radiation, the counter of rents is around 1000\n",
        "\n",
        "\n",
        "7. We can see from the above plot that even if it rains a lot the demand of of rent bikes is not decreasing, here for example even if we have 20 mm of rain there is a big peak of rented bikes\n",
        "\n",
        "8. We can see from the plot that, on the y-axis, the amount of rented bike is very low When we have more than 4 cm of snow, the bike rents is much lower"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. People like to ride bikes when it is pretty hot around 25¬∞C in average. So its add positive impact to the business.\n",
        "\n",
        "2. In between 20% to 80% humidity that show a positive lead to the business and below 20%and above 80% humidity lead to negative business growth. \n",
        "\n",
        "\n",
        "3. The demand of rented bike is uniformly distribute despite of wind speed but when the speed of wind was 7 m/s then the demand of rented bike also increase that clearly means peoples love to ride bikes when its little windy.that is good sign of positive business growth.\n",
        "\n",
        "4. The day visibility was below 500 that leads to negative growth in the business.\n",
        "\n",
        "6. The amount of rented bikes is huge, when there is solar radiation, the counter of rents is around 1000.It was a positive insights in the business. \n",
        "\n",
        "7. In the time of raining the demend of bike increasing and its give a positive impect in the business.\n",
        "\n",
        "8. The amount of rented bike is very low When we have more than 4 cm of snow, the bike rents is much lower.It shows that in winter the business is not profitable. The insights that lead to negative growth in the business."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 percentage distribution of the value counts of the categorical features"
      ],
      "metadata": {
        "id": "AbVyPk0_Y35M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# percentage distribution of the value counts of the categorical features\n",
        "cols=['Month','Holiday','Seasons','Hour','weekdays_weekend']\n",
        "n=1\n",
        "plt.figure(figsize=(20,15))\n",
        "for i in cols:\n",
        "  plt.subplot(3,3,n)\n",
        "  n=n+1\n",
        "  plt.pie(bike_df1[i].value_counts(),labels = bike_df1[i].value_counts().keys().tolist(),autopct='%.0f%%')\n",
        "  plt.title(i)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "RiL5OBfeY3ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "data_corr= bike_df1.corr()       \n",
        "sns.heatmap(data_corr, cmap='bwr', linewidths=0.1, annot=True, linecolor='black')\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation heatmaps can be used to find potential relationships between variables and to understand the strength of these relationships."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can infer the following from the above correlation heatmap -\n",
        "\n",
        "1. Temperature and Dew point temperature are highly correlated to each other.\n",
        "\n",
        "2. We see a positive correlation between Rented bike count and temperature. Note that this is only true for the range of temperatures provided.\n",
        "\n",
        "3. We see a negative correlation among rented bike count with humidity,Rainfall and Snowfall. The more the humidity, Rainfall and Snowfall the less people prefer to bike.\n",
        "\n",
        "4. visibility has a weak dependence on Humidity.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9- Pair Plot "
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "bike_df1.head(2)"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(15, 8))\n",
        "sns.pairplot(bike_df1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q9T2mQMc6JhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to understand the best set of features to explain a relationship between two variables or to form the most separated clusters"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can infer the following from the above Pair Plot -\n",
        "\n",
        "1. When the snofall and Rainfall increased Rented bike count decreased.\n",
        "\n",
        "2. With the increased of Temperature Rented bike count also increased.\n",
        "\n",
        "3. There is a positive  relation between count and visibility.\n",
        "\n",
        "4. Visibility has a weak dependence on Humidity\n",
        "\n",
        "5. When the snofall and Rainfall are decreased solar radiation increased."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Regression plot"
      ],
      "metadata": {
        "id": "HmyMsH5Ypllj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in bike_df1.select_dtypes(include=[\"float\"]).columns:\n",
        "  fig,ax=plt.subplots(figsize=(10,6))\n",
        "  sns.regplot(x=bike_df1[col],y=bike_df1['Rented_Bike_Count'],scatter_kws={\"color\": 'orange'}, line_kws={\"color\": \"black\"})"
      ],
      "metadata": {
        "id": "ZJc3HolBpkcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####  What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "9RWkaxfwgtJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ***From the above regression plot of all numerical features we see that the columns  'Temperature', 'Wind_speed','Visibility','Solar_Radiation' are positively relation to the target variable.***\n",
        "\n",
        "\n",
        "* ***which means the rented bike count increases with increase of these features.***\n",
        "* ***'Rainfall','Snowfall','Humidity' these features are negatively related with the target variaable which means the rented bike count decreases when these features increase.***"
      ],
      "metadata": {
        "id": "V5oEVGpHggGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1.head(2)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "missing = pd.DataFrame((bike_df1.isnull().mean())*100).reset_index()"
      ],
      "metadata": {
        "id": "d61r4sLegEJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing"
      ],
      "metadata": {
        "id": "iYuE195EgINi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10,5))\n",
        "ax = sns.pointplot(x='index',y=0,data=missing)\n",
        "plt.xticks(rotation =90,fontsize =7)\n",
        "plt.title(\"Percentage of Missing values\")\n",
        "plt.ylabel(\"PERCENTAGE\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SGRaUBBbg_Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see above there are no missing value presents thankfully"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.Checking multicollinearity between variables"
      ],
      "metadata": {
        "id": "QIKM3mYmUy7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ordinary least squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable**"
      ],
      "metadata": {
        "id": "Qfx6iQSeWxLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the module\n",
        "#assign the 'x','y' value\n",
        "import statsmodels.api as sm\n",
        "X = bike_df1[[ 'Temperature','Humidity',\n",
        "       'Wind_speed', 'Visibility','Dew_point_temperature',\n",
        "       'Solar_Radiation', 'Rainfall', 'Snowfall']]\n",
        "Y = bike_df1['Rented_Bike_Count']"
      ],
      "metadata": {
        "id": "Nd6q7T5MU0kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1.head()"
      ],
      "metadata": {
        "id": "ntGQpgeRU0r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add a constant column\n",
        "X = sm.add_constant(X)\n",
        "X"
      ],
      "metadata": {
        "id": "cLCdtPnVU0u6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## fit a OLS model \n",
        "\n",
        "model= sm.OLS(Y, X).fit()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "iVyYm-itU0yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.corr()"
      ],
      "metadata": {
        "id": "eeg8f_lDWKud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* **R sqauare and Adj Square are near to each other. 40% of variance in the Rented Bike count is  explained by the model.**\n",
        "*  **For F statistic , P value is less than 0.05 for 5% levelof significance.**\n",
        "*  **P value of dew point temp and visibility are very high and they are not significant.**\n",
        "\n",
        "*  **Omnibus tests the skewness and kurtosis of the residuals. Here the value of Omnibus is high., it shows we have skewness in our data.**\n",
        "*  **The condition number is large, 3.11e+04. This might indicate that there are strong multicollinearity or other numerical problems**   \n",
        "*  **Durbin-Watson tests for autocorrelation of the residuals. Here value is less than 0.5. We can say that there exists a positive auto correlation among the variables.**"
      ],
      "metadata": {
        "id": "MivJ0KGNWmy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***From the OLS model we find that the 'Temperature' and  'Dew_point_temperature' are highly correlated so we need to drop one of them.***\n",
        "* ***for droping the we check the (P>|t|) value from above table and we can see that the 'Dew_point_temperature' value is higher so we need to drop Dew_point_temperature column***\n",
        "* ***For clarity, we use visualisation i.e heatmap in next step***\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PbxeDuT-yOct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "data_corr= bike_df1.corr()       \n",
        "sns.heatmap(data_corr, cmap='bwr', linewidths=0.1, annot=True, linecolor='black')\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F_Jagqq_XjV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***We can observe on the heatmap that on the target variable line the most positively correlated variables to the rent are :***\n",
        "\n",
        "* the temperature\n",
        "* the dew point temperature\n",
        "* the solar radiation\n",
        "\n",
        "***And most negatively correlated variables are:***\n",
        "* Humidity\n",
        "* Rainfall\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J6tv49pSXWYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***From the above correlation heatmap, We see that there is a positive \n",
        "correlation between columns 'Temperature' and 'Dew point temperature' i.e 0.91 so even if we drop this column then it dont affects the outcome of our analysis. And they have the same variations.. so we can drop the column 'Dew point temperature(¬∞C)'.***"
      ],
      "metadata": {
        "id": "lv0G1-S0XWhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#drop the Dew point temperature column\n",
        "bike_df1=bike_df1.drop(['Dew_point_temperature'],axis=1)"
      ],
      "metadata": {
        "id": "wvq6sOv6Xdjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#show 1st 2 row of dataset\n",
        "bike_df1.head(2)"
      ],
      "metadata": {
        "id": "eBh78m4--oZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1.info()"
      ],
      "metadata": {
        "id": "eX25HZFdYqoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# draw boxplot of numeric values of the dataset\n",
        "plt.figure(figsize=(18, 18))\n",
        "for i, col in enumerate(bike_df1.select_dtypes(include=[\"float\"]).columns):\n",
        "    ax = plt.subplot(3,3, i+1)\n",
        "    sns.boxplot(data=bike_df1, x=col, ax=ax,color='r')\n",
        "plt.suptitle('Box Plot of continuous variables')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "sCm_kxc9EUYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the IQR\n",
        "bike_df1_cap=bike_df1.copy()"
      ],
      "metadata": {
        "id": "MCAWTG1UKON3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find feature of the dataset\n",
        "feature=bike_df1_cap.loc[:,['Temperature', 'Humidity', 'Wind_speed','Visibility', 'Solar_Radiation']]"
      ],
      "metadata": {
        "id": "UC33EnY0w4XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature"
      ],
      "metadata": {
        "id": "ql469WHhxRrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing the outliner\n",
        "def iqr_capping(df,cols,factor):                 # this function remover the outliner of dataset with iql method \n",
        "  for col in cols:\n",
        "    q1=df[col].quantile(0.25)\n",
        "    q3=df[col].quantile(0.75)\n",
        "\n",
        "    iqr=q3-q1\n",
        "\n",
        "    upper_whisker=q3+(factor*iqr)\n",
        "    lower_whisker=q1-(factor*iqr)\n",
        "\n",
        "    df[col]=np.where(df[col]>upper_whisker,upper_whisker,np.where(df[col]<lower_whisker,lower_whisker,df[col]))"
      ],
      "metadata": {
        "id": "PULkSt-GKOQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#call the function\n",
        "iqr_capping(bike_df1_cap,feature,1.5)"
      ],
      "metadata": {
        "id": "8hMdIVlzKOTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1_cap"
      ],
      "metadata": {
        "id": "_FcPtFstx4V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final check of outliner in dataset\n",
        "plt.figure(figsize=(18, 18))\n",
        "for i, col in enumerate(['Temperature', 'Humidity', 'Wind_speed','Visibility', 'Solar_Radiation']):\n",
        "    plt.subplot(3,2, i+1)\n",
        "    sns.boxplot(data=bike_df1_cap, x=col,color='r')\n",
        "plt.suptitle('Box Plot of continuous variables')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "LmySyBT0xwd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1_cap.describe().T"
      ],
      "metadata": {
        "id": "cN9vT8urJVIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Now which is show that the dataset has no outliner.***"
      ],
      "metadata": {
        "id": "UCUjtcgl5z4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Interquartile Range Definition***\n",
        "\n",
        "\n",
        "The interquartile range defines the difference between the third and the first quartile. Quartiles are the partitioned values that divide the whole series into 4 equal parts. So, there are 3 quartiles. First Quartile is denoted by Q1 known as the lower quartile, the second Quartile is denoted by Q2 and the third Quartile is denoted by Q3 known as the upper quartile. Therefore, the interquartile range is equal to the upper quartile minus lower quartile.\n",
        "\n",
        "***Interquartile Range Formula***\n",
        "The difference between the upper and lower quartile is known as the interquartile range. The formula for the interquartile range is given below\n",
        "\n",
        "***Interquartile range(IQR)*** = Upper Quartile ‚Äì Lower Quartile = Q¬≠3 ‚Äì Q¬≠1\n",
        "\n",
        "* where Q1 is the first quartile and Q3 is the third quartile of the series.\n",
        "\n",
        "Median: In the box plot, the median is displayed rather than the mean.\n",
        "* Q1: The first quartile (25%) position.\n",
        "* Q3: The third quartile (75%) position.\n",
        "* Interquartile range (IQR): a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles. It represents how 50% of the points were dispersed.\n",
        "* Lower and upper 1.5*IQR whiskers: These represent the limits and boundaries for the outliers.\n",
        "* Outliers: Defined as observations that fall below Q1 ‚àí 1.5 IQR or above Q3 + 1.5 IQR. Outliers are displayed as dots or circles."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Data Scaling"
      ],
      "metadata": {
        "id": "TP7f1HdGyvdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.boxplot(bike_df1_cap['Rented_Bike_Count'],vert=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K_842pJ-ztiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply diffrent tranformation technique and checking data distributation\n",
        "fig,axes = plt.subplots(1,4,figsize=(20,5))\n",
        "sns.distplot((bike_df1_cap['Rented_Bike_Count']),ax=axes[0],color='brown').set_title(\" Input data\")\n",
        "sns.distplot(np.log1p(bike_df1_cap['Rented_Bike_Count']+0.0000001),ax=axes[1],color='red').set_title(\"log1p\") #transform only posible in positive value and >0 value so add 0.0000001 in data\n",
        "sns.distplot(np.sqrt(bike_df1_cap['Rented_Bike_Count']),ax=axes[2], color='blue').set_title(\"Square root\")\n",
        "sns.distplot(np.cbrt(bike_df1_cap['Rented_Bike_Count']*2),ax=axes[3], color='green').set_title(\"cube root\")"
      ],
      "metadata": {
        "id": "yOESLZrty2BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats"
      ],
      "metadata": {
        "id": "iwLz5vk8y2EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plotvariable(df,variable):\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.subplot(1,2,1)   #means 1 row, 2 Columns and 1st plot\n",
        "  df[variable].hist(bins=30)\n",
        "\n",
        "  ##QQ plot\n",
        "  plt.subplot(1,2,2)\n",
        "  stats.probplot(df[variable], dist='norm',plot=plt)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "NA-sufHgy2HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1_cap.head(2)"
      ],
      "metadata": {
        "id": "vXbJeyeky2Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotvariable(bike_df1_cap,'Rented_Bike_Count')"
      ],
      "metadata": {
        "id": "xoe1JGL_zPgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "our Rented Bike Count target is not normally distributed ,so we need to make some transformations before supply to the model"
      ],
      "metadata": {
        "id": "9tXH3WXBzTyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1[\"RBC_qb\"]=np.cbrt(bike_df['Rented_Bike_Count']) \n",
        "#try cube root technique to convert positive screwd to normal distributation\n",
        "plotvariable(bike_df1,\"RBC_qb\")"
      ],
      "metadata": {
        "id": "xX1L5kdazPjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.boxplot(bike_df1[\"RBC_qb\"],vert=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ivonoD1SzPmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1_cap['Rented_Bike_Count']=np.sqrt(bike_df1_cap['Rented_Bike_Count'])\n",
        "\n",
        "plotvariable(bike_df1_cap,'Rented_Bike_Count')"
      ],
      "metadata": {
        "id": "oQm7W5EDzPpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.boxplot(bike_df1_cap[\"Rented_Bike_Count\"],vert=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hvkyfhzIzPro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, Its Look Like Normal Distributation"
      ],
      "metadata": {
        "id": "OEkzeyvCzf3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plotvariable(bike_df1_cap,'Wind_speed')"
      ],
      "metadata": {
        "id": "FH591Pd9zPuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "not look like normal and right screwed distributation so need to apply transformation"
      ],
      "metadata": {
        "id": "rWSC5SGrzlT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1_cap['Wind_speed']=np.sqrt(bike_df1_cap['Wind_speed'])\n",
        "\n",
        "plotvariable(bike_df1_cap,'Wind_speed')"
      ],
      "metadata": {
        "id": "s9cMFpHTzPyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "look like a normal distribution."
      ],
      "metadata": {
        "id": "qJ5oFxruzrWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### What all scaling techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "xOKFSDUPRxL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***In here I used square root,Cube root and log transformation***\n",
        "\n",
        "1. Square Root The square root method is typically used when your data is moderately skewed. Now using the square root (e.g., sqrt(x)) is a transformation that has a moderate effect on distribution shape. It is generally used to reduce right skewed data. Finally, the square root can be applied on zero values and is most commonly used on counted data.\n",
        "\n",
        "\n",
        "    Square Root Transformation: Transform the values from y to ‚àöy.\n",
        "\n",
        "\n",
        "\n",
        "2. Log Transformation The logarithmic is a strong transformation that has a major effect on distribution shape. This technique is, as the square root method, oftenly used for reducing right skewness. Worth noting, however, is that it can not be applied to zero or negative values.\n",
        "\n",
        "\n",
        "    Log Transformation: Transform the values from y to log(y).\n",
        "\n",
        "\n",
        "\n",
        "3. Cube root transformation involves converting x to x^ (1/3). This is a fairly strong transformation with a substantial effect on distribution shape: but is weaker than the logarithm. It can be applied to negative and zero values too. Negatively skewed data\n",
        "\n",
        "\n",
        "    Cube Root Transformation: Transform the values from y to y^(1/3)"
      ],
      "metadata": {
        "id": "xRwKdjuxTKJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iQgau7-4TKYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "CQg1AhKrNA7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features=list(bike_df1_cap.select_dtypes(['category']).columns)\n",
        "categorical_features=pd.Index(categorical_features)\n",
        "categorical_features"
      ],
      "metadata": {
        "id": "3tfRWyAR0iIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1_cap_copy = bike_df1_cap\n",
        "\n",
        "def one_hot_encoding(data, column):\n",
        "    data = pd.concat([data, pd.get_dummies(data[column], prefix=column, drop_first=True)], axis=1)\n",
        "    data = data.drop([column], axis=1)\n",
        "    return data\n",
        "\n",
        "for col in categorical_features:\n",
        "    bike_df1_cap_copy = one_hot_encoding(bike_df1_cap_copy, col)\n",
        "bike_df1_cap_copy.head()      "
      ],
      "metadata": {
        "id": "SDu33HFbdi4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1_cap_copy.shape"
      ],
      "metadata": {
        "id": "Bc6T1s6B1B7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***In here I used both OdinalEncoder on 'Seasons' feature and OneHotEncoder on 'Hour','Holiday','Month','weekdays_weekend' features.***\n",
        "\n",
        "* OdinalEncoder is used when the variables in the data are ordinal, ordinal encoding converts each label into integer values and the encoded data represents the sequence of labels.\n",
        "\n",
        "* In One-Hot Encoding, each category of any categorical variable gets a new variable. It maps each category with binary numbers (0 or 1). This type of encoding is used when the data is nominal. Newly created binary features can be considered dummy variables. After one hot encoding, the number of dummy variables depends on the number of categories presented in the data."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6**. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import Library\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV,  cross_val_score"
      ],
      "metadata": {
        "id": "9J4a2pi4xV5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "bike_df1_cap_copy.head(2)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test= train_test_split(bike_df1_cap_copy.drop([\"Rented_Bike_Count\"],axis=1),bike_df1_cap_copy[\"Rented_Bike_Count\"],test_size=0.25,random_state=2)"
      ],
      "metadata": {
        "id": "MivCJfUrv_wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.head(2)"
      ],
      "metadata": {
        "id": "5cKu0FTxwNvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.tail(2)"
      ],
      "metadata": {
        "id": "8ZYxLoKPwNzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "id": "8Mnk16PbwN2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "id": "dhbRJtBbwN5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "lD4lNuQC2gAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "id": "p6uvUAxz2gEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The foregoing data splitting methods can be implemented once we specify a splitting ratio. A commonly used ratio is 80:20, which means 80% of the data is for training and 20% for testing which I did in here. Other ratios such as 70:30, 60:40, and even 50:50 are also used in practice. There does not seem to be clear guidance on what ratio is best or optimal for a given dataset. The 80:20 split draws its justification from the well-known Pareto principle, but that is again just a thumb rule used by practitioners."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* R2 shows how well terms (data points) fit a curve or line. Adjusted R2 also indicates how well terms fit a curve or line, but adjusts for the number of terms in a model. If you add more and more useless variables to a model, adjusted r-squared will decrease. If you add more useful variables, adjusted r-squared will increase.\n",
        "\n",
        "\n",
        "\n",
        "* Adjusted R2 will always be less than or equal to R2.\n",
        "\n",
        "* R2 assumes that every single variable explains the variation in the dependent variable.\n",
        "\n",
        "\n",
        "* The adjusted R2 tells you the percentage of variation explained by only the independent variables that actually affect the dependent variable.\n",
        "\n",
        "\n",
        "* MSE is a risk function that allows us to calculate the average squared difference between a feature‚Äôs or variable‚Äôs predicted and actual value.\n",
        "\n",
        "\n",
        "* RMSE is an abbreviation for Root Mean Square Error, which is the square root of the value obtained from the Mean Square Error function."
      ],
      "metadata": {
        "id": "MYrVDu7LVyUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model-1. LINEAR REGRESSION"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression analysis is used to predict the value of a variable based on the value of another variable. The variable you want to predict is called the dependent variable. The variable you are using to predict the other variable's value is called the independent variable.\n",
        "\n",
        "This form of analysis estimates the coefficients of the linear equation, involving one or more independent variables that best predict the value of the dependent variable. Linear regression fits a straight line or surface that minimizes the discrepancies between predicted and actual output values. There are simple linear regression calculators that use a ‚Äúleast squares‚Äù method to discover the best-fit line for a set of paired data. You then estimate the value of X (dependent variable) from Y (independent variable)."
      ],
      "metadata": {
        "id": "Ms_ibZHfWql-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "#import the packages\n",
        "from sklearn.linear_model import LinearRegression\n",
        "reg= LinearRegression().fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the score\n",
        "reg.score(x_train, y_train)"
      ],
      "metadata": {
        "id": "6uKvLPew2669"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks like our r2 score value is 0.63 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**\n",
        "\n"
      ],
      "metadata": {
        "id": "bM009uZ8-1Wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reg.coef_"
      ],
      "metadata": {
        "id": "kw3vfDlM2692"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get the x_train and X-test value\n",
        "y_pred_train=reg.predict(x_train)\n",
        "y_pred_test=reg.predict(x_test)"
      ],
      "metadata": {
        "id": "YK4Jydqk27AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
        "#calculate MSE\n",
        "MSE_lr= mean_squared_error((y_train), (y_pred_train))\n",
        "print(\"MSE :\",MSE_lr)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_lr=np.sqrt(MSE_lr)\n",
        "print(\"RMSE :\",RMSE_lr)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_lr= mean_absolute_error(y_train, y_pred_train)\n",
        "print(\"MAE :\",MAE_lr)\n",
        "\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_lr= r2_score(y_train, y_pred_train)\n",
        "print(\"R2 :\",r2_lr)\n",
        "Adjusted_R2_lr = (1-(1-r2_score(y_train, y_pred_train))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "ChezbAFR27DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks like our r2 score value is 0.63 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "Toffn_ap3hrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Linear regression ',\n",
        "       'MAE':round((MAE_lr),3),\n",
        "       'MSE':round((MSE_lr),3),\n",
        "       'RMSE':round((RMSE_lr),3),\n",
        "       'R2_score':round((r2_lr),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_lr ),2)\n",
        "       }\n",
        "training_df=pd.DataFrame(dict1,index=[1])"
      ],
      "metadata": {
        "id": "sacnz8NH27Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
        "#calculate MSE\n",
        "MSE_lr= mean_squared_error(y_test, y_pred_test)\n",
        "print(\"MSE :\",MSE_lr)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_lr=np.sqrt(MSE_lr)\n",
        "print(\"RMSE :\",RMSE_lr)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_lr= mean_absolute_error(y_test, y_pred_test)\n",
        "print(\"MAE :\",MAE_lr)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_lr= r2_score((y_test), (y_pred_test))\n",
        "print(\"R2 :\",r2_lr)\n",
        "Adjusted_R2_lr = (1-(1-r2_score((y_test), (y_pred_test)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)))\n",
        "print(\"Adjusted R2 :\",Adjusted_R2_lr )\n"
      ],
      "metadata": {
        "id": "nYkRyOSP27JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The r2_score for the test set is 0.60. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G92kN1RQ3p9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Linear regression ',\n",
        "       'MAE':round((MAE_lr),3),\n",
        "       'MSE':round((MSE_lr),3),\n",
        "       'RMSE':round((RMSE_lr),3),\n",
        "       'R2_score':round((r2_lr),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_lr ),2)\n",
        "       }\n",
        "test_df=pd.DataFrame(dict2,index=[1])"
      ],
      "metadata": {
        "id": "14-mG4Kt3tp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test),(y_test)-(y_pred_test))"
      ],
      "metadata": {
        "id": "dh6y1_AL3tse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(y_pred_test)\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel('No of Test Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GOOdsufv3tvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "cross_val_score(reg,x_train, y_train,cv=30)"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model-2. LASSO REGRESSION"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of multicollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.\n",
        "\n",
        "Lasso Regression uses L1 regularization technique (will be discussed later in this article). It is used when we have more features because it automatically performs feature selection."
      ],
      "metadata": {
        "id": "orgLuVJ7WnW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Create an instance of Lasso Regression implementation\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso = Lasso(alpha=1.0, max_iter=3000)\n",
        "# Fit the Lasso model\n",
        "lasso.fit(x_train, y_train)\n",
        "# Create the model score\n",
        "print(lasso.score(x_test, y_test), lasso.score(x_train, y_train))"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the x_train and X-test value\n",
        "y_pred_train_lasso=lasso.predict(x_train)\n",
        "y_pred_test_lasso=lasso.predict(x_test)"
      ],
      "metadata": {
        "id": "WS0M6neJcBO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_l= mean_squared_error((y_train), (y_pred_train_lasso))\n",
        "print(\"MSE :\",MSE_l)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_l=np.sqrt(MSE_l)\n",
        "print(\"RMSE :\",RMSE_l)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_l= mean_absolute_error(y_train, y_pred_train_lasso)\n",
        "print(\"MAE :\",MAE_l)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_l= r2_score(y_train, y_pred_train_lasso)\n",
        "print(\"R2 :\",r2_l)\n",
        "Adjusted_R2_l = (1-(1-r2_score(y_train, y_pred_train_lasso))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_lasso))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "R0FLx_u-cBSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks like our r2 score value is 0.40 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**\n",
        "\n"
      ],
      "metadata": {
        "id": "5ZR9d47Z-pav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Lasso regression ',\n",
        "       'MAE':round((MAE_l),3),\n",
        "       'MSE':round((MSE_l),3),\n",
        "       'RMSE':round((RMSE_l),3),\n",
        "       'R2_score':round((r2_l),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_l ),2)\n",
        "       }\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "4Zc2d505cBWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_l= mean_squared_error(y_test, y_pred_test_lasso)\n",
        "print(\"MSE :\",MSE_l)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_l=np.sqrt(MSE_l)\n",
        "print(\"RMSE :\",RMSE_l)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_l= mean_absolute_error(y_test, y_pred_test_lasso)\n",
        "print(\"MAE :\",MAE_l)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_l= r2_score((y_test), (y_pred_test_lasso))\n",
        "print(\"R2 :\",r2_l)\n",
        "Adjusted_R2_l=(1-(1-r2_score((y_test), (y_pred_test_lasso)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_lasso)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "gnJt0m8QcBZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Lasso regression ',\n",
        "       'MAE':round((MAE_l),3),\n",
        "       'MSE':round((MSE_l),3),\n",
        "       'RMSE':round((RMSE_l),3),\n",
        "       'R2_score':round((r2_l),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_l ),2),\n",
        "       }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "DdVHywKKcBcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(np.array(y_pred_test_lasso))\n",
        "plt.plot(np.array((y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lAAYPBVpcBfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_lasso),(y_test-y_pred_test_lasso))"
      ],
      "metadata": {
        "id": "hJHJuUsRcUAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3. RIDGE REGRESSION"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge regression is a specialized technique used to analyze multiple regression data that is multicollinear in nature. It is a fundamental regularization technique, but it is not used very widely because of the complex science behind it. However, it is fairly easy to explore the science behind ridge regression in r if you have an overall idea of the concept of multiple regression. Regression stays the same, but in regularization, the way the model coefficients are determined is different.\n",
        "\n",
        "The main idea of ridge regression focuses on fitting a new line that does not fit."
      ],
      "metadata": {
        "id": "mB4-ZxpuXMTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "#import the packages\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge= Ridge(alpha=0.1)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FIT THE MODEL\n",
        "ridge.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "aX3Bry0Jeq-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the score\n",
        "ridge.score(x_train, y_train)"
      ],
      "metadata": {
        "id": "E7LZagjTerBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the x_train and X-test value\n",
        "y_pred_train_ridge=ridge.predict(x_train)\n",
        "y_pred_test_ridge=ridge.predict(x_test)"
      ],
      "metadata": {
        "id": "vpd7bdBUerEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_ridge),(y_test)-(y_pred_test_ridge))"
      ],
      "metadata": {
        "id": "Oy9Ur-i2erXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot((y_pred_test_ridge))\n",
        "plt.plot((np.array(y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xLPcQYVderTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Ridge regression ',\n",
        "       'MAE':round((MAE_r),3),\n",
        "       'MSE':round((MSE_r),3),\n",
        "       'RMSE':round((RMSE_r),3),\n",
        "       'R2_score':round((r2_r),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_r ),2)}\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "NdW2AZpKerQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_r= mean_squared_error(y_test, y_pred_test_ridge)\n",
        "print(\"MSE :\",MSE_r)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_r=np.sqrt(MSE_r)\n",
        "print(\"RMSE :\",RMSE_r)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_r= mean_absolute_error(y_test, y_pred_test_ridge)\n",
        "print(\"MAE :\",MAE_r)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_r= r2_score((y_test), (y_pred_test_ridge))\n",
        "print(\"R2 :\",r2_r)\n",
        "Adjusted_R2_r=(1-(1-r2_score((y_test), (y_pred_test_ridge)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_ridge)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "UTwR5FC8erN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_r= mean_squared_error((y_train), (y_pred_train_ridge))\n",
        "print(\"MSE :\",MSE_r)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_r=np.sqrt(MSE_r)\n",
        "print(\"RMSE :\",RMSE_r)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_r= mean_absolute_error(y_train, y_pred_train_ridge)\n",
        "print(\"MAE :\",MAE_r)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_r= r2_score(y_train, y_pred_train_ridge)\n",
        "print(\"R2 :\",r2_r)\n",
        "Adjusted_R2_r=(1-(1-r2_score(y_train, y_pred_train_ridge))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_ridge))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "6YgQbz7MerHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks like our r2 score value is 0.63 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**\n",
        "\n"
      ],
      "metadata": {
        "id": "gxOqsjiI-ieF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Ridge regression ',\n",
        "       'MAE':round((MAE_r),3),\n",
        "       'MSE':round((MSE_r),3),\n",
        "       'RMSE':round((RMSE_r),3),\n",
        "       'R2_score':round((r2_r),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_r ),2)}\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "Lb_XvFzCerKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ML Model-4.RANDOM FOREST"
      ],
      "metadata": {
        "id": "xswJzNwGvtMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Regression is a supervised learning algorithm that uses ensemble learning method for regression. Ensemble learning method is a technique that combines predictions from multiple machine learning algorithms to make a more accurate prediction than a single model."
      ],
      "metadata": {
        "id": "CEqzwOfvXPIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# Create an instance of the RandomForestRegressor\n",
        "rf_model = RandomForestRegressor()\n",
        "\n",
        "rf_model.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "R6204BBTvvQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on train and test data\n",
        "\n",
        "y_pred_train_r = rf_model.predict(x_train)\n",
        "y_pred_test_r = rf_model.predict(x_test)"
      ],
      "metadata": {
        "id": "9jjhMjfEvvcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Model Score:\",rf_model.score(x_train,y_train))\n",
        "\n",
        "#calculate MSE\n",
        "MSE_rf= mean_squared_error(y_train, y_pred_train_r)\n",
        "print(\"MSE :\",MSE_rf)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_rf=np.sqrt(MSE_rf)\n",
        "print(\"RMSE :\",RMSE_rf)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_rf= mean_absolute_error(y_train, y_pred_train_r)\n",
        "print(\"MAE :\",MAE_rf)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_rf= r2_score(y_train, y_pred_train_r)\n",
        "print(\"R2 :\",r2_rf)\n",
        "Adjusted_R2_rf=(1-(1-r2_score(y_train, y_pred_train_r))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_r))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "5p4gKQOuvvnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks like our r2 score value is 0.97 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**\n",
        "\n"
      ],
      "metadata": {
        "id": "UeIEjdlZv7IN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Random forest regression ',\n",
        "       'MAE':round((MAE_rf),3),\n",
        "       'MSE':round((MSE_rf),3),\n",
        "       'RMSE':round((RMSE_rf),3),\n",
        "       'R2_score':round((r2_rf),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_rf ),2)}\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "K4RO2EE_v3dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_rf= mean_squared_error(y_test, y_pred_test_r)\n",
        "print(\"MSE :\",MSE_rf)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_rf=np.sqrt(MSE_rf)\n",
        "print(\"RMSE :\",RMSE_rf)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_rf= mean_absolute_error(y_test, y_pred_test_r)\n",
        "print(\"MAE :\",MAE_rf)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_rf= r2_score((y_test), (y_pred_test_r))\n",
        "print(\"R2 :\",r2_rf)\n",
        "Adjusted_R2_rf=(1-(1-r2_score((y_test), (y_pred_test_r)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_r)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "WmKMY_I6v3m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The r2_score for the test set is 0.74. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**"
      ],
      "metadata": {
        "id": "ZNPFkh5iwDYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Random forest regression ',\n",
        "       'MAE':round((MAE_rf),3),\n",
        "       'MSE':round((MSE_rf),3),\n",
        "       'RMSE':round((RMSE_rf),3),\n",
        "       'R2_score':round((r2_rf),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_rf ),2)}\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "Np7jN0V5wE_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_r),(y_test)-(y_pred_test_r))"
      ],
      "metadata": {
        "id": "nJdR2gMEwOzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model.feature_importances_"
      ],
      "metadata": {
        "id": "FCcarcrqwO-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = rf_model.feature_importances_\n",
        "\n",
        "importance_dict = {'Feature' : list(x_train.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)"
      ],
      "metadata": {
        "id": "5aJ9vHjVzlcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"
      ],
      "metadata": {
        "id": "LF2NY5M9zlge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "metadata": {
        "id": "R_YONBMRztxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FIT THE MODEL\n",
        "rf_model.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "RUgchZDfzt4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = x_train.columns\n",
        "importances = rf_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "mjLz0z0qzt7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,20))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1msZre-Bzt_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ML Model-5.Gtadient Boosting"
      ],
      "metadata": {
        "id": "oeDJ5wgnJ5iU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient boosting Regression calculates the difference between the current prediction and the known correct target value. This difference is called residual. After that Gradient boosting Regression trains a weak model that maps features to that residual."
      ],
      "metadata": {
        "id": "MbefRhJlXbD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "# Create an instance of the GradientBoostingRegressor\n",
        "gb_model = GradientBoostingRegressor()\n",
        "\n",
        "\n",
        "gb_model.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "EzJwa_oMJ_tQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred_train_g = gb_model.predict(x_train)\n",
        "y_pred_test_g = gb_model.predict(x_test)"
      ],
      "metadata": {
        "id": "wdbpHTg3J_vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Model Score:\",gb_model.score(x_train,y_train))\n",
        "#calculate MSE\n",
        "MSE_gb= mean_squared_error(y_train, y_pred_train_g)\n",
        "print(\"MSE :\",MSE_gb)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_gb=np.sqrt(MSE_gb)\n",
        "print(\"RMSE :\",RMSE_gb)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_gb= mean_absolute_error(y_train, y_pred_train_g)\n",
        "print(\"MAE :\",MAE_gb)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_gb= r2_score(y_train, y_pred_train_g)\n",
        "print(\"R2 :\",r2_gb)\n",
        "Adjusted_R2_gb = (1-(1-r2_score(y_train, y_pred_train_g))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_g))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "367uAtQ2J_zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks like our r2 score value is 0.74 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "7k9-6uvXKZtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Gradient boosting regression ',\n",
        "       'MAE':round((MAE_gb),3),\n",
        "       'MSE':round((MSE_gb),3),\n",
        "       'RMSE':round((RMSE_gb),3),\n",
        "       'R2_score':round((r2_gb),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gb ),2),\n",
        "       }\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "CEnPhW9BJ_4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_gb= mean_squared_error(y_test, y_pred_test_g)\n",
        "print(\"MSE :\",MSE_gb)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_gb=np.sqrt(MSE_gb)\n",
        "print(\"RMSE :\",RMSE_gb)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_gb= mean_absolute_error(y_test, y_pred_test_g)\n",
        "print(\"MAE :\",MAE_gb)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_gb= r2_score((y_test), (y_pred_test_g))\n",
        "print(\"R2 :\",r2_gb)\n",
        "Adjusted_R2_gb = (1-(1-r2_score((y_test), (y_pred_test_g)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)))\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_g)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "f5Mp9lndJ_7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The r2_score for the test set is 0.69. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "256fvQdTKjnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Gradient boosting regression ',\n",
        "       'MAE':round((MAE_gb),3),\n",
        "       'MSE':round((MSE_gb),3),\n",
        "       'RMSE':round((RMSE_gb),3),\n",
        "       'R2_score':round((r2_gb),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gb ),2),\n",
        "       }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "ZcYhhTbCQglu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_g),(y_test)-(y_pred_test_g))"
      ],
      "metadata": {
        "id": "6yx321mJKABJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_model.feature_importances_"
      ],
      "metadata": {
        "id": "wOJ8aUGGK0Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01-iaFqne5gM"
      },
      "outputs": [],
      "source": [
        "importances = gb_model.feature_importances_\n",
        "\n",
        "importance_dict = {'Feature' : list(x_train.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"
      ],
      "metadata": {
        "id": "5molvQViK0Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.head()"
      ],
      "metadata": {
        "id": "zR_9KKf_K0ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "metadata": {
        "id": "NUC2vo38K0bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_model.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "G8OVE22GK0ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = x_train.columns\n",
        "importances = gb_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "NLjwRezPK0h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,20))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oJj7E7NvLQs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hyperparameter tuning** "
      ],
      "metadata": {
        "id": "YW5s2LDMLoOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before proceding to try next models, let us try to tune some hyperparameters and see if the performance of our model improves.\n",
        "\n",
        "Hyperparameter tuning is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a model argument whose value is set before the learning process begins. The key to machine learning algorithms is hyperparameter tuning."
      ],
      "metadata": {
        "id": "hnJ7FLCkLrZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Using GridSearchCV"
      ],
      "metadata": {
        "id": "qzSZOM5xLrcz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV helps to loop through predefined hyperparameters and fit the model on the training set. So, in the end, we can select the best parameters from the listed hyperparameters."
      ],
      "metadata": {
        "id": "kl4K-STsLxf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Gradient Boosting Regressor with GridSearchCV**"
      ],
      "metadata": {
        "id": "qZh0cG_2L07D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Provide the range of values for chosen hyperparameters**"
      ],
      "metadata": {
        "id": "9PwzBNCIL4BW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of trees\n",
        "n_estimators = [50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}"
      ],
      "metadata": {
        "id": "ipD5YtviLQvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_dict"
      ],
      "metadata": {
        "id": "c48Ks4NWLQxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "# Create an instance of the GradientBoostingRegressor\n",
        "gb_model = GradientBoostingRegressor()\n",
        "\n",
        "# Grid search\n",
        "gb_grid = GridSearchCV(estimator=gb_model,\n",
        "                       param_grid = param_dict,\n",
        "                       cv = 5, verbose=2)\n",
        "\n",
        "gb_grid.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "Jg1H3pJ-LQ0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_grid.best_estimator_"
      ],
      "metadata": {
        "id": "ydasxdwILQ3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_optimal_model = gb_grid.best_estimator_"
      ],
      "metadata": {
        "id": "bRwMR8sNLQ5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on train and test data\n",
        "\n",
        "y_pred_train_g_g = gb_optimal_model.predict(x_train)\n",
        "y_pred_g_g= gb_optimal_model.predict(x_test)"
      ],
      "metadata": {
        "id": "7oTXtsG_LQ9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Model Score:\",gb_optimal_model.score(x_train,y_train))\n",
        "MSE_gbh= mean_squared_error(y_train, y_pred_train_g_g)\n",
        "print(\"MSE :\",MSE_gbh)\n",
        "\n",
        "RMSE_gbh=np.sqrt(MSE_gbh)\n",
        "print(\"RMSE :\",RMSE_gbh)\n",
        "\n",
        "\n",
        "MAE_gbh= mean_absolute_error(y_train, y_pred_train_g_g)\n",
        "print(\"MAE :\",MAE_gbh)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2_gbh= r2_score(y_train, y_pred_train_g_g)\n",
        "print(\"R2 :\",r2_gbh)\n",
        "Adjusted_R2_gbh = (1-(1-r2_score(y_train, y_pred_train_g_g))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_g_g))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "me9nuIarMR4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Gradient Boosting gridsearchcv ',\n",
        "       'MAE':round((MAE_gbh),3),\n",
        "       'MSE':round((MSE_gbh),3),\n",
        "       'RMSE':round((RMSE_gbh),3),\n",
        "       'R2_score':round((r2_gbh),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gbh ),2)\n",
        "      }\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "7YqlbkJLMR6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "MSE_gbh= mean_squared_error(y_test, y_pred_g_g)\n",
        "print(\"MSE :\",MSE_gbh)\n",
        "\n",
        "RMSE_gbh=np.sqrt(MSE_gbh)\n",
        "print(\"RMSE :\",RMSE_gbh)\n",
        "\n",
        "\n",
        "MAE_gbh= mean_absolute_error(y_test, y_pred_g_g)\n",
        "print(\"MAE :\",MAE_gbh)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2_gbh= r2_score((y_test), (y_pred_g_g))\n",
        "print(\"R2 :\",r2_gbh)\n",
        "Adjusted_R2_gbh = (1-(1-r2_score(y_test, y_pred_g_g))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_g_g)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "M-dpt7dmMR-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Gradient Boosting gridsearchcv ',\n",
        "       'MAE':round((MAE_gbh),3),\n",
        "       'MSE':round((MSE_gbh),3),\n",
        "       'RMSE':round((RMSE_gbh),3),\n",
        "       'R2_score':round((r2_gbh),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gbh ),2)\n",
        "      }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "w7NpkHPOMSF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_g_g),(y_test)-(y_pred_g_g))"
      ],
      "metadata": {
        "id": "mBj--lJyMpLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_optimal_model.feature_importances_"
      ],
      "metadata": {
        "id": "AAjZ4rVEMpTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = gb_optimal_model.feature_importances_\n",
        "\n",
        "importance_dict = {'Feature' : list(x_train.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)"
      ],
      "metadata": {
        "id": "WEONko9fMpbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"
      ],
      "metadata": {
        "id": "_1t69WSgMunj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.head()"
      ],
      "metadata": {
        "id": "HX99fvZZMuq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "metadata": {
        "id": "j1uyK9aWM0ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_model.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "_db4j3HUM03n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = x_train.columns\n",
        "importances = gb_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "eqxlU8BNLP4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,20))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JYJjMz_VLQDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# displaying the results of evaluation metric values for all models\n",
        "result=pd.concat([training_df,test_df],keys=['Training set','Test set'])\n",
        "result"
      ],
      "metadata": {
        "id": "iOrD_hAOYVQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above dataframe we can chose Gradient Boosting regression model for final prediction model because this model is perform better compare to other model.Random forest regression was also perform good in this model. "
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Gradient boosting regression model for feature importance because it is more accurate compared to other model."
      ],
      "metadata": {
        "id": "tCb8Vij7YbSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the time of our analysis, we initially did EDA on all the features of our datset. We first analysed our dependent variable, 'Rented Bike Count' and also transformed it. Next we analysed categorical variable and dropped the variable who had majority of one class, we also analysed numerical variable, found out the correlation, distribution and their relationship with the dependent variable. We also removed some numerical features who had mostly 0 values and hot encoded the categorical variables.\n",
        "\n",
        "Next we implemented 5 machine learning algorithms Linear Regression,lasso,ridge,Random Forest and XGBoost. We did hyperparameter tuning to improve our model performance. The results of our evaluation are:"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# displaying the results of evaluation metric values for all models\n",
        "result=pd.concat([training_df,test_df],keys=['Training set','Test set'])\n",
        "result"
      ],
      "metadata": {
        "id": "hV-LkWpJOBdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "‚Ä¢ No overfitting is seen.\n",
        "\n",
        "‚Ä¢ Random forest Regressor and Gradient Boosting gridsearchcv gives the highest R2 score of 97% and 84% recpectively for Train Set and 75% for Test set.\n",
        "\n",
        "‚Ä¢ Feature Importance value for Random Forest and Gradient Boost are different.\n",
        "\n",
        "‚Ä¢ We can deploy this model."
      ],
      "metadata": {
        "id": "_XdAsPaxOLdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}